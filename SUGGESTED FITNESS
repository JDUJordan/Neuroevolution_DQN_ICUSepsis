import numpy as np
import torch
from src.algos.dqn import QNetwork, load_model

def evaluate_modified_dqn(env, modified_weights, num_eval_episodes=10, render=False):
    """
    Evaluate DQN with modified weights.
    
    Args:
        env: Gym environment
        modified_weights (dict): Modified state_dict for the Q-network
        num_eval_episodes (int): Number of episodes to evaluate
        render (bool): Whether to render the environment
    
    Returns:
        dict: Dictionary containing evaluation metrics
    """
    # Initialize network and load weights
    q_network = QNetwork(env).to('cuda' if torch.cuda.is_available() else 'cpu')
    q_network.load_state_dict(modified_weights)
    q_network.eval()  # Set to evaluation mode
    
    episode_rewards = []
    episode_lengths = []
    success_count = 0
    
    for episode in range(num_eval_episodes):
        state = env.reset()
        episode_reward = 0
        steps = 0
        done = False
        
        while not done:
            # Convert state to tensor
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(q_network.device)
            
            # Get action from network (no exploration)
            with torch.no_grad():
                action = q_network(state_tensor).argmax().item()
            
            # Take step in environment
            next_state, reward, done, info = env.step(action)
            
            if render:
                env.render()
            
            episode_reward += reward
            steps += 1
            state = next_state
            
            # Optional: Check for success condition
            if 'success' in info:
                if info['success']:
                    success_count += 1
        
        episode_rewards.append(episode_reward)
        episode_lengths.append(steps)
    
    # Calculate metrics
    metrics = {
        'mean_reward': np.mean(episode_rewards),
        'std_reward': np.std(episode_rewards),
        'min_reward': np.min(episode_rewards),
        'max_reward': np.max(episode_rewards),
        'mean_length': np.mean(episode_lengths),
        'success_rate': success_count / num_eval_episodes if success_count else None
    }
    
    return metrics

# Example usage:
def get_fitness(modified_weights):
    """
    Convert evaluation metrics to single fitness score.
    Use this as your fitness function for the swarm optimization.
    """
    metrics = evaluate_modified_dqn(env, modified_weights)
    
    # You can adjust these weights based on what you care about most
    fitness = (
        0.7 * metrics['mean_reward'] +  # Primary focus on average performance
        0.2 * (-metrics['std_reward']) +  # Penalty for high variance
        0.1 * (-metrics['mean_length'])   # Small penalty for longer episodes
    )
    
    return fitness

# Example of how to use:
if __name__ == "__main__":
    # Load your environment
    env = gym.make('YourEnv-v0')
    
    # Load base weights (your trained model)
    base_model = QNetwork(env)
    base_model.load_state_dict(torch.load('models/dqn/your_trained_model.pt'))
    
    # Example of evaluating base model
    base_metrics = evaluate_modified_dqn(
        env=env,
        modified_weights=base_model.state_dict(),
        num_eval_episodes=10,
        render=True
    )
    
    print("Base Model Metrics:", base_metrics)
    
    # This function can now be used in your swarm optimization
    # fitness = get_fitness(modified_weights)