import os
import sys
sys.path.append(os.getcwd())  # Add current directory to Python path
import torch
import torch.nn as nn
import gymnasium as gym
import numpy as np
from src.utils.utils import set_seeds
from torchsummary import summary

def encode_state(obs, n_states):  # Modified to take n_states as parameter
    obs = torch.Tensor(obs)
    return nn.functional.one_hot(obs.long(), n_states).float()

def get_mask(info, n_actions=25):
    allowed_actions = info['admissible_actions']
    mask = np.zeros(n_actions)
    mask[allowed_actions] = 1
    return torch.Tensor(mask).unsqueeze(0)

class QNetwork(nn.Module):
    def __init__(self, env):
        super().__init__()
        self.n_states = env.observation_space.n
        self.n_actions = env.action_space.n
        self.network = nn.Linear(self.n_states, self.n_actions, bias=False)
        torch.nn.init.constant_(self.network.weight, 0.0)

    def forward(self, x, action_masks=None):
        q_values = self.network(x)
        if action_masks is not None:
            q_values = q_values - ((1 - action_masks) * 1e10)
        return q_values

def evaluate_network(num_episodes=10000, render=False):
    # Create environment
    env = gym.make('Sepsis/ICU-Sepsis-v2')
    
    # Create network and load weights
    q_network = QNetwork(env)
    state_dict = torch.load("models/dqn/dqn_seed_0_episode_499900.pt")
    q_network.load_state_dict(state_dict)
    q_network.eval()


    # Print the summary
    # Print the summary
    summary(q_network, (1, env.observation_space.n))


    episode_rewards = []
    episode_lengths = []
    
    for episode in range(num_episodes):
        state, info = env.reset()
        episode_reward = 0
        steps = 0
        done = False
        
        while not done:
            # Prepare state and action mask
            state_encoded = encode_state(np.array([state]), env.observation_space.n)  # Pass correct state size
            action_mask = get_mask(info, n_actions=env.action_space.n)
            
            # Get action from network
            with torch.no_grad():
                q_values = q_network(state_encoded, action_mask)
                action = torch.argmax(q_values).item()
            
            # Take step in environment
            next_state, reward, terminated, truncated, info = env.step(action)
            
            episode_reward += reward
            steps += 1
            state = next_state
            done = terminated or truncated

        episode_rewards.append(episode_reward)
        episode_lengths.append(steps)
        print(f"Episode {episode}: Reward = {episode_reward}, Length = {steps}")
    
    env.close()
    
    print("\nEvaluation Results:")
    print(f"Average Reward: {np.mean(episode_rewards):.6f} ± {np.std(episode_rewards):.2f}")
    print(f"Average Length: {np.mean(episode_lengths):.2f} ± {np.std(episode_lengths):.2f}")
    
    return episode_rewards, episode_lengths

if __name__ == "__main__":
    set_seeds(0)
    rewards, lengths = evaluate_network(num_episodes=10)
    print("Rewards:", rewards)
    print("Lengths:", lengths)